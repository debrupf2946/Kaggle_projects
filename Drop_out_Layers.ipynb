{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2ca17c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp311-cp311-macosx_12_0_arm64.whl (1.9 kB)\n",
      "Collecting tensorflow-macos==2.13.0 (from tensorflow)\n",
      "  Downloading tensorflow_macos-2.13.0-cp311-cp311-macosx_12_0_arm64.whl (189.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=2.9.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading h5py-3.9.0-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl (20.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.3/400.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (67.6.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading wrapt-1.15.0-cp311-cp311-macosx_11_0_arm64.whl (36 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading grpcio-1.56.0-cp311-cp311-macosx_10_10_universal2.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.14,>=2.13 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.14,>=2.13.1 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.40.0)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.26.16)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.3)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wrapt, werkzeug, typing-extensions, termcolor, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, markdown, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow-macos, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 h5py-3.9.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-macos-2.13.0 termcolor-2.3.0 typing-extensions-4.5.0 werkzeug-2.3.6 wrapt-1.15.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96c8b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"update\"\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip update -n base -c defaults conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd9e0a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/homebrew/lib/python3.11/site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (67.6.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.56.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/homebrew/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/homebrew/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d142402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6102bfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "883642f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.2-cp311-cp311-macosx_11_0_arm64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.1.0-cp311-cp311-macosx_11_0_arm64.whl (229 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.41.0-cp311-cp311-macosx_10_9_universal2.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.4-cp311-cp311-macosx_11_0_arm64.whl (63 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (9.5.0)\n",
      "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.41.0 kiwisolver-1.4.4 matplotlib-3.7.2 pyparsing-3.0.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfdac9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.0-cp311-cp311-macosx_12_0_arm64.whl (9.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.1 scikit-learn-1.3.0 threadpoolctl-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b835a8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib-inline in /opt/homebrew/lib/python3.11/site-packages (0.1.6)\n",
      "Requirement already satisfied: traitlets in /opt/homebrew/lib/python3.11/site-packages (from matplotlib-inline) (5.9.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install  matplotlib-inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e8e06b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter_contrib_nbextensions\n",
      "  Downloading jupyter_contrib_nbextensions-0.7.0.tar.gz (23.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ipython_genutils in /opt/homebrew/lib/python3.11/site-packages (from jupyter_contrib_nbextensions) (0.2.0)\n",
      "Collecting jupyter_contrib_core>=0.3.3 (from jupyter_contrib_nbextensions)\n",
      "  Downloading jupyter_contrib_core-0.4.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jupyter_core in /Users/debruppaul/Library/Python/3.11/lib/python/site-packages (from jupyter_contrib_nbextensions) (5.3.0)\n",
      "Collecting jupyter_highlight_selected_word>=0.1.1 (from jupyter_contrib_nbextensions)\n",
      "  Downloading jupyter_highlight_selected_word-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting jupyter_nbextensions_configurator>=0.4.0 (from jupyter_contrib_nbextensions)\n",
      "  Downloading jupyter_nbextensions_configurator-0.6.3-py2.py3-none-any.whl (466 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.9/466.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nbconvert>=6.0 in /opt/homebrew/lib/python3.11/site-packages (from jupyter_contrib_nbextensions) (7.6.0)\n",
      "Requirement already satisfied: notebook>=6.0 in /opt/homebrew/lib/python3.11/site-packages (from jupyter_contrib_nbextensions) (6.5.4)\n",
      "Requirement already satisfied: tornado in /Users/debruppaul/Library/Python/3.11/lib/python/site-packages (from jupyter_contrib_nbextensions) (6.3.2)\n",
      "Requirement already satisfied: traitlets>=4.1 in /opt/homebrew/lib/python3.11/site-packages (from jupyter_contrib_nbextensions) (5.9.0)\n",
      "Requirement already satisfied: lxml in /opt/homebrew/lib/python3.11/site-packages (from jupyter_contrib_nbextensions) (4.9.2)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from jupyter_contrib_core>=0.3.3->jupyter_contrib_nbextensions) (67.6.1)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/lib/python3.11/site-packages (from jupyter_nbextensions_configurator>=0.4.0->jupyter_contrib_nbextensions) (6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (6.0.0)\n",
      "Requirement already satisfied: defusedxml in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (3.1.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (0.2.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (3.0.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (0.8.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (5.9.1)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (23.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (2.15.1)\n",
      "Requirement already satisfied: tinycss2 in /opt/homebrew/lib/python3.11/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (1.2.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/debruppaul/Library/Python/3.11/lib/python/site-packages (from jupyter_core->jupyter_contrib_nbextensions) (3.5.3)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/debruppaul/Library/Python/3.11/lib/python/site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (25.1.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/homebrew/lib/python3.11/site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (21.3.0)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in /Users/debruppaul/Library/Python/3.11/lib/python/site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (8.2.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /Users/debruppaul/Library/Python/3.11/lib/python/site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (1.5.6)\n",
      "Requirement already satisfied: ipykernel in /Users/debruppaul/Library/Python/3.11/lib/python/site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (6.23.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/homebrew/lib/python3.11/site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/homebrew/lib/python3.11/site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (0.17.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/homebrew/lib/python3.11/site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (0.17.1)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /opt/homebrew/lib/python3.11/site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (1.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/homebrew/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert>=6.0->jupyter_contrib_nbextensions) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /opt/homebrew/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert>=6.0->jupyter_contrib_nbextensions) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from jupyter-client>=5.3.4->notebook>=6.0->jupyter_contrib_nbextensions) (2.8.2)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in /opt/homebrew/lib/python3.11/site-packages (from nbclassic>=0.4.7->notebook>=6.0->jupyter_contrib_nbextensions) (2.7.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /opt/homebrew/lib/python3.11/site-packages (from nbclassic>=0.4.7->notebook>=6.0->jupyter_contrib_nbextensions) (0.2.3)\n",
      "Requirement already satisfied: fastjsonschema in /opt/homebrew/lib/python3.11/site-packages (from nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (2.17.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/homebrew/lib/python3.11/site-packages (from nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (4.18.3)\n",
      "Requirement already satisfied: ptyprocess in /opt/homebrew/lib/python3.11/site-packages (from terminado>=0.8.3->notebook>=6.0->jupyter_contrib_nbextensions) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/homebrew/lib/python3.11/site-packages (from argon2-cffi->notebook>=6.0->jupyter_contrib_nbextensions) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.11/site-packages (from beautifulsoup4->nbconvert>=6.0->jupyter_contrib_nbextensions) (2.4.1)\n",
      "Requirement already satisfied: appnope in /opt/homebrew/lib/python3.11/site-packages (from ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.1.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/debruppaul/Library/Python/3.11/lib/python/site-packages (from ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/debruppaul/Library/Python/3.11/lib/python/site-packages (from ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/homebrew/lib/python3.11/site-packages (from ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (8.14.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/homebrew/lib/python3.11/site-packages (from ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.1.6)\n",
      "Requirement already satisfied: psutil in /Users/debruppaul/Library/Python/3.11/lib/python/site-packages (from ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (5.9.5)\n",
      "Requirement already satisfied: backcall in /opt/homebrew/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/homebrew/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/homebrew/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /opt/homebrew/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/homebrew/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (3.0.39)\n",
      "Requirement already satisfied: stack-data in /opt/homebrew/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (4.8.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (2023.6.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (0.29.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (0.8.10)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.0->jupyter_contrib_nbextensions) (3.7.1)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.0->jupyter_contrib_nbextensions) (0.6.3)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.0->jupyter_contrib_nbextensions) (0.4.4)\n",
      "Requirement already satisfied: overrides in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.0->jupyter_contrib_nbextensions) (7.3.1)\n",
      "Requirement already satisfied: websocket-client in /opt/homebrew/lib/python3.11/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.0->jupyter_contrib_nbextensions) (1.6.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=6.0->jupyter_contrib_nbextensions) (1.15.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.0->jupyter_contrib_nbextensions) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.0->jupyter_contrib_nbextensions) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=6.0->jupyter_contrib_nbextensions) (2.21)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/homebrew/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.8.3)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/homebrew/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.0->jupyter_contrib_nbextensions) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/homebrew/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.0->jupyter_contrib_nbextensions) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/homebrew/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.0->jupyter_contrib_nbextensions) (0.1.1)\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/homebrew/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /opt/homebrew/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.2.2)\n",
      "Requirement already satisfied: fqdn in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (2.4)\n",
      "Requirement already satisfied: uri-template in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (1.13)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/homebrew/lib/python3.11/site-packages (from isoduration->jsonschema>=2.6->nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (1.2.3)\n",
      "Building wheels for collected packages: jupyter_contrib_nbextensions, jupyter_contrib_core\n",
      "  Building wheel for jupyter_contrib_nbextensions (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jupyter_contrib_nbextensions: filename=jupyter_contrib_nbextensions-0.7.0-py2.py3-none-any.whl size=23428785 sha256=739fb8ad2ff927e2c45a17599ac606a898a78efd195a5eca83eabff191ddc3a6\n",
      "  Stored in directory: /Users/debruppaul/Library/Caches/pip/wheels/cd/25/fe/cb6f3e82f5b1921b0157ac9e32adb2e54806ec1befc446be21\n",
      "  Building wheel for jupyter_contrib_core (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jupyter_contrib_core: filename=jupyter_contrib_core-0.4.2-py2.py3-none-any.whl size=17484 sha256=5eaff486c3dc1c901a8e4dcc029e07fca7c97aa6777b1b46774d2e1a9fc9d336\n",
      "  Stored in directory: /Users/debruppaul/Library/Caches/pip/wheels/37/c3/18/be7a983c1120f15dc0c2d1cb9c33749871a93b034185e00ced\n",
      "Successfully built jupyter_contrib_nbextensions jupyter_contrib_core\n",
      "Installing collected packages: jupyter_highlight_selected_word, jupyter_contrib_core, jupyter_nbextensions_configurator, jupyter_contrib_nbextensions\n",
      "Successfully installed jupyter_contrib_core-0.4.2 jupyter_contrib_nbextensions-0.7.0 jupyter_highlight_selected_word-0.2.0 jupyter_nbextensions_configurator-0.6.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install  jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e345b963",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (860091544.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python3.11 -m pip install --upgrade pip\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python3.11 -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0f4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed7367d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.linspace(-1,1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe5ce19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([-0.6561 , -0.3099 , -0.59035, -0.50855, -0.285  , \n",
    "                    -0.2443 , -0.02445,  0.00135, -0.2006 ,  0.07475, \n",
    "                    -0.1422 ,  0.06515,  0.15265,  0.3521 ,  0.28415,  \n",
    "                    0.5524 ,  0.23115,  0.20835, 0.4211,  0.60485])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa08e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.linspace(-1,1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d24a2071",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([-0.69415, -0.451  , -0.43005, -0.4484 , -0.1475 ,\n",
    "                   -0.5019 , -0.28055,  0.24595, -0.21425, -0.0286 ,  \n",
    "                   0.23415,  0.46575, 0.07955,  0.1973 ,  0.0719 ,\n",
    "                   0.3639 ,  0.5536 ,  0.3365 , 0.50705,  0.33435])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93674858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAGdCAYAAADaPpOnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6l0lEQVR4nO3de3xU9Z3/8fdkIAkIk8ACSSCjkUu5lJtCSUMbwZIK4lpsZEVgFSgL3a4oiPgrdFvw0hXaUhtq2dK6ovbxULCy8fKwiKVoKrIpIIIXBBQaJMQkSFMTLkpg8v39MZ2RIZPLJHM5Z+b1fDzmEec73zPzPXMSzsfv5fN1GGOMAAAAbCIp1g0AAAAIBcELAACwFYIXAABgKwQvAADAVgheAACArRC8AAAAWyF4AQAAtkLwAgAAbKVDrBsQbg0NDfr444/VtWtXORyOWDcHAAC0gjFGp06dUu/evZWU1HzfStwFLx9//LHcbnesmwEAANqgvLxc2dnZzdaJu+Cla9eukrwn73K5YtwaAADQGnV1dXK73f77eHPiLnjxDRW5XC6CFwAAbKY1Uz6YsAsAAGyF4AUAANgKwQsAALCVuJvz0hrGGF24cEEejyfWTbElp9OpDh06sBQdABATCRe81NfXq7KyUmfPno11U2ytc+fOysrKUnJycqybAgBIMAkVvDQ0NKisrExOp1O9e/dWcnIyvQchMsaovr5en3zyicrKyjRgwIAWkwkBABBOCRW81NfXq6GhQW63W507d451c2yrU6dO6tixoz766CPV19crNTU11k0CACSQhPxfZnoK2o/vEAAQKwnV8wIAANrB45G2b5cqK6WsLCk/X3I6o94MghcAANCy4mJp4ULp+PEvyrKzpTVrpMLCqDaFvv8ElJOTo6Kiolg3AwBgF8XF0tSpgYGLJFVUeMuLi6PanKgEL2vXrlVOTo5SU1OVm5urXbt2NVv/008/1R133KGsrCylpKToS1/6kjZv3hyNplrW+PHjtWjRorC81+7duzV//vywvBcAIM55PN4eF2Mav+YrW7TIWy9KIj5s9Mwzz2jx4sVat26dcnNzVVRUpIkTJ+rQoUPq1atXo/r19fX65je/qV69emnTpk3q06ePPvroI6Wnp0e6qaGxyLifjzFGHo9HHTq0fEl79uwZhRYBAOLC9u2Ne1wuZoxUXu6tN358VJoU8Z6Xhx9+WPPmzdOcOXM0ZMgQrVu3Tp07d9b69euD1l+/fr1qamr0/PPP62tf+5pycnI0btw4jRgxItJNbb3iYiknR7r2WmnGDO/PnJyIdZvNnj1bf/7zn7VmzRo5HA45HA498cQTcjgcevnllzVq1CilpKTojTfe0JEjRzRlyhRlZGSoS5cu+spXvqI//elPAe936bCRw+HQ//zP/+jb3/62OnfurAEDBujFF1+MyLkAAGymsjK89cIgosFLfX299uzZo4KCgi8+MClJBQUFKi0tDXrMiy++qLy8PN1xxx3KyMjQ0KFD9dBDDzWZyv/cuXOqq6sLeERUDMb91qxZo7y8PM2bN0+VlZWqrKyU2+2WJC1dulSrVq3SgQMHNHz4cJ0+fVqTJ0/Wtm3btHfvXk2aNEk33nijjh071uxn3H///brlllv0zjvvaPLkyZo5c6ZqamrCfi4AAJvJygpvvTCIaPBy8uRJeTweZWRkBJRnZGSoqqoq6DF//etftWnTJnk8Hm3evFk/+tGP9POf/1w//vGPg9ZfuXKl0tLS/A/fTT0iYjTul5aWpuTkZHXu3FmZmZnKzMyU8x9DVA888IC++c1vql+/furevbtGjBih7373uxo6dKgGDBigBx98UP369WuxJ2X27NmaPn26+vfvr4ceekinT59ucW4SACAB5Od7VxU1lZHe4ZDcbm+9KLHcaqOGhgb16tVLv/3tbzVq1ChNmzZN//mf/6l169YFrb9s2TLV1tb6H+Xl5ZFrXCjjflEyevTogOenT5/WkiVLNHjwYKWnp6tLly46cOBAiz0vw4cP9//3ZZddJpfLpRMnTkSkzQAAG3E6vcuhpcYBjO95UVFU531GdMJujx495HQ6VV1dHVBeXV2tzMzMoMdkZWWpY8eO/p4FSRo8eLCqqqpUX1/faCPAlJQUpaSkhL/xwVhw3O+yyy4LeL5kyRJt3bpVq1evVv/+/dWpUydNnTpV9fX1zb5Px44dA547HA41NDSEvb0AABsqLJQ2bQqe56WoKL7yvCQnJ2vUqFHatm2bv6yhoUHbtm1TXl5e0GO+9rWv6fDhwwE3zg8++MAaOxjHcNwvOTm5yXk/F9uxY4dmz56tb3/72xo2bJgyMzN19OjRsLcHAJBgCgulo0el116Tnn7a+7OsLOqBixSFYaPFixfr0Ucf1ZNPPqkDBw7oe9/7ns6cOaM5c+ZIkm6//XYtW7bMX/973/ueampqtHDhQn3wwQf6wx/+oIceekh33HFHpJvashiO++Xk5Gjnzp06evSoTp482WSvyIABA1RcXKx9+/bp7bff1owZM+hBAQCEh9PpXQ49fbr3Z4xShEQ8eJk2bZpWr16t5cuXa+TIkdq3b5+2bNnin8R77NgxVV40zOJ2u/XKK69o9+7dGj58uO666y4tXLhQS5cujXRTWxbDcb8lS5bI6XRqyJAh6tmzZ5NzWB5++GF169ZNY8eO1Y033qiJEyfq6quvDnt7AACIFYcxwZbO2FddXZ3S0tJUW1srl8sV8Nrnn3+usrIyXXnllUpNTW37hwTb38Htjsm4X6yE7bsEAEDN378vxcaMbVFYKE2ZYqkMuwCQMCKc4dxiCdQRBMFLW/nG/QAA0RPhnY0ttHEymmG5PC8AAAQV4QznFts4Gc0geAEAWF+EM5xbcONkNIPgBQBgfRHOcG7BBOpoBsELAMD6Ipzh3IIJ1NEMghcAgPVFOMO5BTdORjMIXgAA1hfhDOcW3DgZzSB4AQBYX4QznFtw42Q0g+DFJsaPH69FixaF7f1mz56tm266KWzvBwAR59vZuE+fwPLsbG95OxOxRPjtEUYkqWsjMjACQAxEOMM5CdTtgZ6XNigulnJypGuvlWbM8P7MyYlcAqPZs2frz3/+s9asWSOHwyGHw6GjR4/qvffe0/XXX68uXbooIyNDt912m06ePOk/btOmTRo2bJg6deqkf/qnf1JBQYHOnDmj++67T08++aReeOEF//uVlJREpvEAEG4R3tnYIhsnoxkELyGKRQbGNWvWKC8vT/PmzVNlZaUqKyvVtWtXfeMb39BVV12lN998U1u2bFF1dbVuueUWSVJlZaWmT5+u73znOzpw4IBKSkpUWFgoY4yWLFmiW265RZMmTfK/39ixY8PfcABAVHk8UkmJtGGD92e8JtVj2CgELWVgdDi8GRinTAlvpJ6Wlqbk5GR17txZmZmZkqQf//jHuuqqq/TQQw/5661fv15ut1sffPCBTp8+rQsXLqiwsFBXXHGFJGnYsGH+up06ddK5c+f87wcAsLdE2peJnpcQWCkD49tvv63XXntNXbp08T8GDRokSTpy5IhGjBihCRMmaNiwYfqXf/kXPfroo/r73/8e+YYBAKIu0fZlIngJgZUyMJ4+fVo33nij9u3bF/D48MMPdc0118jpdGrr1q16+eWXNWTIED3yyCMaOHCgysrKIt84AEDUJOK+TAQvIYhlBsbk5GR5LvrNu/rqq7V//37l5OSof//+AY/LLrtMkuRwOPS1r31N999/v/bu3avk5GQ999xzQd8PAGBPVhoViBaClxDEMgNjTk6Odu7cqaNHj+rkyZO64447VFNTo+nTp2v37t06cuSIXnnlFc2ZM0cej0c7d+7UQw89pDfffFPHjh1TcXGxPvnkEw0ePNj/fu+8844OHTqkkydP6vz58+FvNAAg4qw0KhAtBC8hiGUGxiVLlsjpdGrIkCHq2bOn6uvrtWPHDnk8Hl133XUaNmyYFi1apPT0dCUlJcnlcun111/X5MmT9aUvfUk//OEP9fOf/1zXX3+9JGnevHkaOHCgRo8erZ49e2rHjh3hbzQAIOIScV8mhzHBRsnsq66uTmlpaaqtrZXL5Qp47fPPP1dZWZmuvPJKpaamtvkzgs3odru9gUu8zehuSri+SwBA+3g83lxjFRXB5704HN5Rg7Iya+esae7+fSmWSrcBGRgBAFbhGxWYOtUbqFwcwMTrvkwEL23ky8AIAECs+fZlCpbnJR5HBQheAKAN2N8MVpNIowIELwAQokTKZAp7SZRRAVYbAUAIEi2TKWBFCRm8xNkCq5jgO0QiSsRMpoAVJVTw0rFjR0nS2bNnY9wS+/N9h77vFEgEiZjJFLCihJrz4nQ6lZ6erhMnTkiSOnfuLEdT6XIRlDFGZ8+e1YkTJ5Seni5nPM4EA5qQiJlMAStKqOBFkjIzMyXJH8CgbdLT0/3fJZAoEjGTKWBFCRe8OBwOZWVlqVevXuzn00YdO3akxwUJybe/WUuZTCOxvxmALyRc8OLjdDq5AQMISSJmMgWsKKEm7AJAe/kymfbpE1iene0tJ88LEHkJ2/MCAG2VSJlMASsieAGANkiUTKaAFRG8AAAQLxJk0y2CFwAA4kECbbrFhF0AAOwuwTbdIngBAMDOEnDTLYIXAADsLAE33SJ4AQDAzhJw062oBC9r165VTk6OUlNTlZubq127drXquI0bN8rhcOimm26KbAMBALCrBNx0K+LByzPPPKPFixdrxYoVeuuttzRixAhNnDixxY0Rjx49qiVLliifTUIAAGiab9Mt3x4Vl3I4JLc7rjbdinjw8vDDD2vevHmaM2eOhgwZonXr1qlz585av359k8d4PB7NnDlT999/v/r27RvpJgIAYF++TbekxgFMnG66FdHgpb6+Xnv27FFBQcEXH5iUpIKCApWWljZ53AMPPKBevXpp7ty5LX7GuXPnVFdXF/AAACChJNimWxFNUnfy5El5PB5lZGQElGdkZOjgwYNBj3njjTf02GOPad++fa36jJUrV+r+++9vb1MBALC3BNp0y1IZdk+dOqXbbrtNjz76qHr06NGqY5YtW6bFixf7n9fV1cntdkeqiQAAWFeCbLoV0eClR48ecjqdqq6uDiivrq5WZmZmo/pHjhzR0aNHdeONN/rLGhoavA3t0EGHDh1Sv379Ao5JSUlRSkpKBFoPAACsKKJzXpKTkzVq1Cht27bNX9bQ0KBt27YpLy+vUf1Bgwbp3Xff1b59+/yPb33rW7r22mu1b98+elQAAEDkh40WL16sWbNmafTo0RozZoyKiop05swZzZkzR5J0++23q0+fPlq5cqVSU1M1dOjQgOPT09MlqVE5AABITBEPXqZNm6ZPPvlEy5cvV1VVlUaOHKktW7b4J/EeO3ZMSUkk+gUAtI7HkxBzUtEMhzHBdnKyr7q6OqWlpam2tlYulyvWzQEAhFFxsXcPwou38snO9qY5ibPVwAknlPs3XR4AAFsoLpamTm28B2FFhbe8uDg27UL0EbwAQKLxeKSSEmnDBu9PjyfWLWqRx+PtcQk2VuArW7TIFqeCMCB4AYBEUlws5eRI114rzZjh/ZmTY/lui+3bG/e4XMwYqbzcWw/xj+AFABKFjcddKivDWw/2RvACAInA5uMuWVnhrQd7I3gBgERg83GX/HzvqqJLN032cTgkt9tbz8psON0ogFXaT/ACAInA5uMuTqd3ObTUOIDxPS8qsna+F5tON/KzUvsJXgAgEcTBuEthobRpk9SnT2B5dra33Mp5Xmw83UiS9dpPkjoASAQej/d/kysqgs97cTi8UUBZmbW7L2S/DLu+r76pUTurf/XRaj9J6gAAgeJh3OUfnE5p/Hhp+nTvT6s32ebTjSzZfoIXAEgUdh53sTGbTzeyZPsjvjEjAMBCCgulKVPsNe5ic3afbmTF9jPnBQCACLL7dKNotZ85LwAAWITdpxtZsf0ELwAARJjdpxtZrf0MGwEAECV2W+Z9qUi2P5T7NxN2AQCIEt8yb7uySvsZNgIAALZCzwsAILzsPjYCyyN4AQCET3GxtHBhYErW7GzvchWrz0qFbTBsBAAID6vt3oe4RfACAGg/j8fb4xJsAauvbNEibz2gnQheAADtZ8Xd+xC3CF4AAO1nxd37ELcIXgAA7WfF3fsQtwheAADtl5/vXVV06eY3Pg6H5HZ76wHtRPACAGg/K+7eh7hF8AIACA+r7d6HuEWSOsDGSGQKyykslKZM4RcTEUXwAtgUiUxhWVbZvQ9xi2EjwIZIZAogkRG8ADZDItPW8XikkhJpwwbvz0T/PoB4QvAC2AyJTFtWXCzl5EjXXivNmOH9mZNDjxRaicjX8gheAJshkWnzGFJDuxD52gLBC2AzJDJtGkNqaBciX9sgeAFshkSmTWNIDW1G5GsrBC+AzZDItGkMqaHNiHxtheAFsCESmQbHkBrajMjXVkhSB9gUiUwb8w2pVVQE7/13OLyvJ+KQGloQrciXtNhhQfAC2BiJTAP5htSmTvUGKhcHMIk+pIYWRCPyJS122DBsBCCuMKSGNon0ZDJWMoVVVIKXtWvXKicnR6mpqcrNzdWuXbuarPvoo48qPz9f3bp1U7du3VRQUNBsfQC4VGGhdPSo9Npr0tNPe3+WlRG4+JCDrQmRinxZyRR2EQ9ennnmGS1evFgrVqzQW2+9pREjRmjixIk6ceJE0PolJSWaPn26XnvtNZWWlsrtduu6665TRUVFpJsKII74htSmT/f+ZKjIixxsLYhE5MtKprBzGBMsFAyf3NxcfeUrX9GvfvUrSVJDQ4PcbrfuvPNOLV26tMXjPR6PunXrpl/96le6/fbbW6xfV1entLQ01dbWyuVytbv9ABAvfCMXl/6r7xsVYVgtQjZs8EaKLXn6aW+0naBCuX9HtOelvr5ee/bsUUFBwRcfmJSkgoIClZaWtuo9zp49q/Pnz6t79+5BXz937pzq6uoCHgCAQIxcxBBr+MMuosHLyZMn5fF4lJGREVCekZGhqqqqVr3H97//ffXu3TsgALrYypUrlZaW5n+43e52txsA4g0jFzFEWuyws/Rqo1WrVmnjxo167rnnlJqaGrTOsmXLVFtb63+Ul5dHuZUAYH3kYIsh0mKHXUSDlx49esjpdKq6ujqgvLq6WpmZmc0eu3r1aq1atUp//OMfNXz48CbrpaSkyOVyBTwAAIEYuYgx1vCHVUSDl+TkZI0aNUrbtm3zlzU0NGjbtm3Ky8tr8rif/vSnevDBB7VlyxaNHj06kk0EgITAyIUFsIY/bCKeYXfx4sWaNWuWRo8erTFjxqioqEhnzpzRnDlzJEm33367+vTpo5UrV0qSfvKTn2j58uV6+umnlZOT458b06VLF3Xp0iXSzQWAuET2YYsgLXZYRHzOy7Rp07R69WotX75cI0eO1L59+7Rlyxb/JN5jx46p8qJB1l//+teqr6/X1KlTlZWV5X+sXr060k0FgLjGyAXiRcTzvEQbeV4AoHnsDQgrCuX+zcaMAGBFEYwwGLmA3RG8AIDVsPsw0CxL53kBgITD7sNAiwheAMAqyOEPtArBCwBYBTn8gVYheAEAqyCHP9AqBC8AYBXk8AdaheAFAKyCHP5AqxC8AIBVsPsw0CoELwBgJeTwB1pEkjoAsJrCQmnKFHL4A00geAEAKyKHP9Akho0AAICtELwAAABbIXgBAAC2QvACAABsheAFAADYCsELAACwFYIXAABgK+R5AQAL8njIUQc0heAFACymuFhauFA6fvyLsuxs77ZH7A4AELwAiBW6FoIqLpamTpWMCSyvqPCW22F7Iy4tIo05LwCir7hYysmRrr1WmjHD+zMnx1uewDweb4/LpYGL9EXZokXeelbFpUU0ELwAiC5f18LFYyLSF10LdrnLeTxSSYm0YYP3Zxgiiu3bG38tFzNGKi/31rOieLm0sD6CFwDREw9dC1LEuhcqK8NbL5ri5dLCHgheAESP3bsWpIh2L2RlhbdeNMXDpYV9ELwAiB47dy1IEe9eyM/3ripyOIK/7nBIbre3ntXY/dLCXgheAESPnbsWpIh3Lzid3uXQUuMAxve8qMiaK3fsfmlhLwQvAKLHzl0LUlS6FwoLvcuh+/QJLM/OtvYyabtfWtgLwQuA6LFz14IUte6FwkLp6FHptdekp5/2/iwrs27gItn/0sJeCF4ARJdduxakqHYvOJ3S+PHS9Onen3a46dv50sJeHMYEm3lmX3V1dUpLS1Ntba1cLlesmwOgKXZNw+pbbSQFTtz1BTTcpW17aRFbody/CV4AIFTBNh9yu73jIgkeuABtFcr9m72NACBUhYXSlCl0LwAxQvACAG3hm5QCIOqYsAsAAGyF4AUAANgKwQsAALAVghcAAGArBC8AAMBWWG0E2BnZwAAkoKj0vKxdu1Y5OTlKTU1Vbm6udu3a1Wz9Z599VoMGDVJqaqqGDRumzZs3R6OZgL0UF0s5OdK110ozZnh/5uR4ywEgjkU8eHnmmWe0ePFirVixQm+99ZZGjBihiRMn6sSJE0Hr/9///Z+mT5+uuXPnau/evbrpppt000036b333ot0UwH78KWovzjDqyRVVHjLCWAAxLGIbw+Qm5urr3zlK/rVr34lSWpoaJDb7dadd96ppUuXNqo/bdo0nTlzRi+99JK/7Ktf/apGjhypdevWtfh5bA+AuOfxeHtYLg1cfBwO7054ZWUMIQGwjVDu3xHteamvr9eePXtUUFDwxQcmJamgoEClpaVBjyktLQ2oL0kTJ05ssv65c+dUV1cX8ADi2vbtTQcuknezwPJybz0AiEMRDV5Onjwpj8ejjIyMgPKMjAxVVVUFPaaqqiqk+itXrlRaWpr/4Xa7w9N4wKoqK8NbDwBsxvZLpZctW6ba2lr/o7y8PNZNAr7g8UglJdKGDd6fHk/73zMrK7z1AMBmIrpUukePHnI6naqurg4or66uVmZmZtBjMjMzQ6qfkpKilJSU8DQYCKfiYmnhwsAhnuxsac0a767EbZWf732figrvENGlfHNe8vPb/hlRwCpvAG0V0Z6X5ORkjRo1Stu2bfOXNTQ0aNu2bcrLywt6TF5eXkB9Sdq6dWuT9QFLiuRqIKfTGwBJ3kDlYr7nRUWWjgRY5Q2gXUyEbdy40aSkpJgnnnjCvP/++2b+/PkmPT3dVFVVGWOMue2228zSpUv99Xfs2GE6dOhgVq9ebQ4cOGBWrFhhOnbsaN59991WfV5tba2RZGprayNyPkCLLlwwJjvbGG+/SOOHw2GM2+2t1x7/+7+NP8ft9pZb2P/+r/crCPa1OByWbz6ACAnl/h3xDLvTpk3TJ598ouXLl6uqqkojR47Uli1b/JNyjx07pqSkLzqAxo4dq6efflo//OEP9YMf/EADBgzQ888/r6FDh0a6qUB4hLIaaPz4tn9OYaE0ZYqtxl48Hu9IWrDRLmO8HUeLFnlPy8KnASDGIp7nJdrI84KY27DBOxbSkqeflqZPj3x7LKSkxDtE1JLXXmtfXAfAfiyT5wVISKwGahKrvAGEA8ELEG6+1UCXTqb1cTgkt9vyq4EigbgOQDgQvADhFgergSKFuA5AOBC8AJFQWCht2iT16RNYnp3tLW9PnhcbI64DEA5M2AUiiUxsQQXL3+d2ewOXBI3rgIQXyv2b4AVATBDXAbhYKPfviOd5AYBgnE6WQwNoG+a8AAAAWyF4AQAAtkLwAgAAbIXgBQAA2ArBCwAAsBWCFwAAYCsELwAAwFYIXgAAgK0QvAAAAFsheAEAALZC8AIAAGyF4AUAANgKwQsAALAVghcAAGArBC8AAMBWCF4AAICtELwAAABbIXgBAAC20iHWDQCAiPB4pO3bpcpKKStLys+XnM5YtwpAGBC8wPK4ByFkxcXSwoXS8eNflGVnS2vWSIWFsWsXgLBg2AiWVlws5eRI114rzZjh/ZmT4y0HgioulqZODQxcJKmiwlvOLw9gewQvsCzuQQiZx+PtcTGm8Wu+skWLvPUA2BbBCyyJexDaZPv2xtHuxYyRysu99QDYFsELLIl7ENqksjK89QBYEsELLIl7ENokKyu89QBYEsELLIl7ENokP9+7qsjhCP66wyG53d56AGyL4AWWxD0IbeJ0epdDS41/eXzPi4pYaw/YHMELLIl7ENqssFDatEnq0yewPDvbW06eF8D2HMYEW89hX3V1dUpLS1Ntba1cLlesm4N2CpZrzO32Bi7cg9AsshsCthLK/ZvgBZbHPQgA4l8o92+2B4DlOZ3S+PGxbgUAwCqY8wIAAGyF4AUAANhKRIOXmpoazZw5Uy6XS+np6Zo7d65Onz7dbP0777xTAwcOVKdOnXT55ZfrrrvuUm1tbSSbCaApHo9UUiJt2OD9yX4MACwgosHLzJkztX//fm3dulUvvfSSXn/9dc2fP7/J+h9//LE+/vhjrV69Wu+9956eeOIJbdmyRXPnzo1kMwEEw5beACwqYquNDhw4oCFDhmj37t0aPXq0JGnLli2aPHmyjh8/rt69e7fqfZ599ln967/+q86cOaMOHVqeX8xqIyAMfFt6X/rPgy/JDvlSAIRZKPfviPW8lJaWKj093R+4SFJBQYGSkpK0c+fOVr+P7yRaE7gACIOLtvT2KEklGqcNulUlGieP+UfwwpbeAGIoYhFBVVWVevXqFfhhHTqoe/fuqqqqatV7nDx5Ug8++GCzQ03nzp3TuXPn/M/r6ura1mAAXv/Y0rtY39ZCrdFxuf0vZatca8xCFZY/563HGnYAMRByz8vSpUvlcDiafRw8eLDdDaurq9MNN9ygIUOG6L777muy3sqVK5WWluZ/uN3uJusCaIXKShXr25qqTTquwBT7FeqjqdqkYn2bLb0BxEzIPS/33HOPZs+e3Wydvn37KjMzUydOnAgov3DhgmpqapSZmdns8adOndKkSZPUtWtXPffcc+rYsWOTdZctW6bFixf7n9fV1RHAAO3g6ZWlhVoj72yXwP+/MUqSQw1apCJN6fVXkegYQCyEHLz07NlTPXv2bLFeXl6ePv30U+3Zs0ejRo2SJL366qtqaGhQbm5uk8fV1dVp4sSJSklJ0YsvvqjU1NRmPyclJUUpKSmhnQSAJm1Xvo43E5YYJalcl2u7+mh89JoFAH4Rm7A7ePBgTZo0SfPmzdOuXbu0Y8cOLViwQLfeeqt/pVFFRYUGDRqkXbt2SfIGLtddd53OnDmjxx57THV1daqqqlJVVZU8TA4EoqLyROv6U1pbDwDCLaJLeJ566iktWLBAEyZMUFJSkm6++Wb98pe/9L9+/vx5HTp0SGfPnpUkvfXWW/6VSP379w94r7KyMuXk5ESyuQDk3fwynPUAINzYVRpAAI/Hm4uuoqJxmhfJm+olO1sqK2N3bwDhY4k8L4BtkAI/gNMprVnj/W9fTjof3/OiIgIXALFD8ILERgr8oAoLvUl0+wSulFZ2Nsl1AcQew0ZIXKTAb5HH481FV1npneOSn0+PC4DICOX+TfCCxOSb2HH8ePDXmdgBAFHFnBegJf9Igd8kY6Tycm89AIClELwgMbU2tT0p8AHAcghekJhIZgIAtkXwgsSUn++d03LpWmAfh0Nyu731AACWQvCCxEQyEwCwLYIXJK4oJDMh/x0AhF9E9zYCLK+wUJoyJSLJTIqLpYULAxc1ZWd7O3wSPH0MALQLeV5gfTbMlEb+OwAIDXleED9smL7f4/H2uAT73wJf2aJFDCEBQFsRvMC6fN0XlyaTq6jwlls0gCH/HQBEFsELrMnG3RfkvwOAyCJ4gTXZuPuC/HcAEFkEL7AmG3dfkP8OACKL4MUqSAgSyMbdF+S/A4DIInixAhuuqIk4m3dfRCH/HQAkLPK8xBoJQZrm+26kwO/HRt+NDVPUAEBMhHL/JniJJY/H28PS1MRUh8P7v+plZYl7xwuWptbt9o67WDxwAQC0Xij3b7YHiKVQVtSMHx+1ZllKBNP3AwDsieAllmy8oiaqnM7EDd4AAI0wYTeWbLyiBgCAWCF4iSWbr6gBACAWCF5iiYQgAACEjOAl1kgIAgBASJiwawWsqAEAoNUIXqyCFTUAALQKw0YAAMBWCF4AAICtELwAAABbIXgBAAC2QvACAABsheAFAADYCsELAACwFYIXAABgKwQvAADAVgheAACArRC8AAAAWyF4AQAAthLR4KWmpkYzZ86Uy+VSenq65s6dq9OnT7fqWGOMrr/+ejkcDj3//PORbCYAALCRiAYvM2fO1P79+7V161a99NJLev311zV//vxWHVtUVCSHwxHJ5gEAABvqEKk3PnDggLZs2aLdu3dr9OjRkqRHHnlEkydP1urVq9W7d+8mj923b59+/vOf680331RWVlakmggAAGwoYj0vpaWlSk9P9wcuklRQUKCkpCTt3LmzyePOnj2rGTNmaO3atcrMzGzxc86dO6e6urqABwAAiF8RC16qqqrUq1evgLIOHTqoe/fuqqqqavK4u+++W2PHjtWUKVNa9TkrV65UWlqa/+F2u9vVbgAAYG0hBy9Lly6Vw+Fo9nHw4ME2NebFF1/Uq6++qqKiolYfs2zZMtXW1vof5eXlbfpsAABgDyHPebnnnns0e/bsZuv07dtXmZmZOnHiRED5hQsXVFNT0+Rw0KuvvqojR44oPT09oPzmm29Wfn6+SkpKGh2TkpKilJSUUE4BAADYWMjBS8+ePdWzZ88W6+Xl5enTTz/Vnj17NGrUKEne4KShoUG5ublBj1m6dKn+7d/+LaBs2LBh+sUvfqEbb7wx1KbiHzweaft2qbJSysqS8vMlpzPWrQIAoG0ittpo8ODBmjRpkubNm6d169bp/PnzWrBggW699Vb/SqOKigpNmDBBv/vd7zRmzBhlZmYG7ZW5/PLLdeWVV0aqqXGtuFhauFA6fvyLsuxsac0aqbAwdu0CAKCtIprn5amnntKgQYM0YcIETZ48WV//+tf129/+1v/6+fPndejQIZ09ezaSzUhYxcXS1KmBgYskVVR4y4uLY9MuAADaw2GMMbFuRDjV1dUpLS1NtbW1crlcsW5OzHg8Uk5O48DFx+Hw9sCUlTGEBACIvVDu3+xtFKe2b286cJEkY6Tycm89AADsJGJzXhBblZXhrdccJgQDAKKJ4MUiwh0AtHZXhfbuvsCEYABAtDFsZAHFxd75KddeK82Y4f2Zk9O+CbX5+d4goqm9LR0Oye321msrJgQDAGKB4CXGIhUAOJ3e3g+pcQDje15U1PbeHY/H2+MSbLq3r2zRIm89AADCieAlhiIdABQWSps2SX36BJZnZ3vL2zOsw4RgAECsMOclhkIJAMaPb9tnFBZKU6aEf0JtNCcEAwBwMYKXGIpWAOB0tj34aUq0JgQDAHApho1iyM4BQDQmBAMAEAzBSwzZOQCI9IRgAACaQvASQ3YPACI5IRgAgKawt5EFBEv05nZ7Axc7BABk2AUAtFco92+CF4sgAAAAJLJQ7t+sNrKISKwIAgAgHjHnBQAA2Ao9L0h4DNkBgL0QvKD9bHz3Z1dsALAfho3QPpHYEjtK2BUbAOyJ4AVtZ+O7P7tiA4B9EbygbWx+92dXbACwL4IXtI3N7/7sig0A9kXwgrax+d3fzptiAkCiI3hB29j87m/nTTEBINERvKBtbH73t/ummACQyAhe0DZxcPdnV2wAsCc2ZkT72H1LbNk6xx4AxA12lSZ4iS7u/gCAdmJXaUQXW2IDAKKIOS8AAMBWCF4AAICtELwAAABbIXgBAAC2QvACAABsheAFAADYCsELAACwFYIXAABgKwQvAADAVgheAACArbA9QCJg7yEAQBwheIl3wXZ9zs6W1qyxza7PAABcjGGjeFZcLE2dGhi4SFJFhbe8uDg27QIAoB0iFrzU1NRo5syZcrlcSk9P19y5c3X69OkWjystLdU3vvENXXbZZXK5XLrmmmv02WefRaqZ8cvj8fa4GNP4NV/ZokXeegAA2EjEgpeZM2dq//792rp1q1566SW9/vrrmj9/frPHlJaWatKkSbruuuu0a9cu7d69WwsWLFBSEh1EIdu+vXGPy8WMkcrLvfUAALCRiMx5OXDggLZs2aLdu3dr9OjRkqRHHnlEkydP1urVq9W7d++gx91999266667tHTpUn/ZwIEDI9HE+FdZGd56AABYRES6NEpLS5Wenu4PXCSpoKBASUlJ2rlzZ9BjTpw4oZ07d6pXr14aO3asMjIyNG7cOL3xxhvNfta5c+dUV1cX8IC8q4rCWQ8AAIuISPBSVVWlXr16BZR16NBB3bt3V1VVVdBj/vrXv0qS7rvvPs2bN09btmzR1VdfrQkTJujDDz9s8rNWrlyptLQ0/8PtdofvROwsP9+7qsjhCP66wyG53d56AADYSEjBy9KlS+VwOJp9HDx4sE0NaWhokCR997vf1Zw5c3TVVVfpF7/4hQYOHKj169c3edyyZctUW1vrf5SXl7fp8+OO0+ldDi01DmB8z4uKyPcCALCdkOa83HPPPZo9e3azdfr27avMzEydOHEioPzChQuqqalRZmZm0OOy/jF8MWTIkIDywYMH69ixY01+XkpKilJSUlrR+gRUWCht2hQ8z0tREXleAAC2FFLw0rNnT/Xs2bPFenl5efr000+1Z88ejRo1SpL06quvqqGhQbm5uUGPycnJUe/evXXo0KGA8g8++EDXX399KM3ExQoLpSlTyLALAIgbEVltNHjwYE2aNEnz5s3TunXrdP78eS1YsEC33nqrf6VRRUWFJkyYoN/97ncaM2aMHA6H7r33Xq1YsUIjRozQyJEj9eSTT+rgwYPatGlTJJqZOJxOafz4WLcCAICwiNj2AE899ZQWLFigCRMmKCkpSTfffLN++ctf+l8/f/68Dh06pLNnz/rLFi1apM8//1x33323ampqNGLECG3dulX9+vWLVDMBAIDNOIwJloLVvurq6pSWlqba2lq5XK5YNwcAALRCKPdvUtcCAABbIXgBAAC2QvACAABsJWITduOOx8NyYwAALIDgpTWKi4MneluzhkRvAABEGcNGLSkulqZODQxcJKmiwlteXBybdgEAkKAIXprj8Xh7XIKtJveVLVrkrQcAAKKC4KU527c37nG5mDFSebm3HgAAiAqCl+ZUVoa3HgAAaDeCl+b8Y6frsNUDAADtRvDSnPx876oihyP46w6H5HZ76wEAgKggeGmO0+ldDi01DmB8z4uKyPcCAEAUEby0pLBQ2rRJnt5ulWicNuhWlWicPH0ulzZtIs8LAABRRpK6VihWoRY6vq3j+qL3JVtGa+QQoQsAANFFz0sLvshRFzhsVFHhIEcdAAAxQPDSDHLUAQBgPQQvzSBHHQAA1kPw0gxy1AEAYD0EL80gRx0AANZD8NIMctQBAGA9BC/NIEcdAADWQ/DSgn/kqFOfPoHl2dnkqAMAIBZIUtcKhYXSlCneVUWVld45Lvn59LgAABALBC+t5HRK48fHuhUAAIBhIwAAYCsELwAAwFYIXgAAgK0QvAAAAFsheAEAALZC8AIAAGyF4AUAANgKwQsAALAVghcAAGArcZdh1xgjSaqrq4txSwAAQGv57tu++3hz4i54OXXqlCTJ7XbHuCUAACBUp06dUlpaWrN1HKY1IY6NNDQ06OOPP1bXrl3lcDjC+t51dXVyu90qLy+Xy+UK63tbQbyfnxT/58j52V+8nyPnZ3+ROkdjjE6dOqXevXsrKan5WS1x1/OSlJSk7OzsiH6Gy+WK219KKf7PT4r/c+T87C/ez5Hzs79InGNLPS4+TNgFAAC2QvACAABsheAlBCkpKVqxYoVSUlJi3ZSIiPfzk+L/HDk/+4v3c+T87M8K5xh3E3YBAEB8o+cFAADYCsELAACwFYIXAABgKwQvAADAVgheLvJf//VfGjt2rDp37qz09PRWHWOM0fLly5WVlaVOnTqpoKBAH374YUCdmpoazZw5Uy6XS+np6Zo7d65Onz4dgTNoWahtOXr0qBwOR9DHs88+668X7PWNGzdG45QCtOW7Hj9+fKO2//u//3tAnWPHjumGG25Q586d1atXL9177726cOFCJE8lqFDPr6amRnfeeacGDhyoTp066fLLL9ddd92l2tragHqxvH5r165VTk6OUlNTlZubq127djVb/9lnn9WgQYOUmpqqYcOGafPmzQGvt+ZvMppCOb9HH31U+fn56tatm7p166aCgoJG9WfPnt3oWk2aNCnSp9GsUM7xiSeeaNT+1NTUgDp2vobB/j1xOBy64YYb/HWsdA1ff/113Xjjjerdu7ccDoeef/75Fo8pKSnR1VdfrZSUFPXv319PPPFEozqh/l2HzMBv+fLl5uGHHzaLFy82aWlprTpm1apVJi0tzTz//PPm7bffNt/61rfMlVdeaT777DN/nUmTJpkRI0aYv/zlL2b79u2mf//+Zvr06RE6i+aF2pYLFy6YysrKgMf9999vunTpYk6dOuWvJ8k8/vjjAfUu/g6ipS3f9bhx48y8efMC2l5bW+t//cKFC2bo0KGmoKDA7N2712zevNn06NHDLFu2LNKn00io5/fuu++awsJC8+KLL5rDhw+bbdu2mQEDBpibb745oF6srt/GjRtNcnKyWb9+vdm/f7+ZN2+eSU9PN9XV1UHr79ixwzidTvPTn/7UvP/+++aHP/yh6dixo3n33Xf9dVrzNxktoZ7fjBkzzNq1a83evXvNgQMHzOzZs01aWpo5fvy4v86sWbPMpEmTAq5VTU1NtE6pkVDP8fHHHzculyug/VVVVQF17HwN//a3vwWc23vvvWecTqd5/PHH/XWsdA03b95s/vM//9MUFxcbSea5555rtv5f//pX07lzZ7N48WLz/vvvm0ceecQ4nU6zZcsWf51Qv7O2IHgJ4vHHH29V8NLQ0GAyMzPNz372M3/Zp59+alJSUsyGDRuMMca8//77RpLZvXu3v87LL79sHA6HqaioCHvbmxOutowcOdJ85zvfCShrzS99pLX1/MaNG2cWLlzY5OubN282SUlJAf/A/vrXvzYul8ucO3cuLG1vjXBdv9///vcmOTnZnD9/3l8Wq+s3ZswYc8cdd/ifezwe07t3b7Ny5cqg9W+55RZzww03BJTl5uaa7373u8aY1v1NRlOo53epCxcumK5du5onn3zSXzZr1iwzZcqUcDe1zUI9x5b+fY23a/iLX/zCdO3a1Zw+fdpfZrVr6NOafwf+3//7f+bLX/5yQNm0adPMxIkT/c/b+521BsNG7VBWVqaqqioVFBT4y9LS0pSbm6vS0lJJUmlpqdLT0zV69Gh/nYKCAiUlJWnnzp1RbW842rJnzx7t27dPc+fObfTaHXfcoR49emjMmDFav359q7Y1D6f2nN9TTz2lHj16aOjQoVq2bJnOnj0b8L7Dhg1TRkaGv2zixImqq6vT/v37w38iTQjX71Jtba1cLpc6dAjc2iza16++vl579uwJ+PtJSkpSQUGB/+/nUqWlpQH1Je+18NVvzd9ktLTl/C519uxZnT9/Xt27dw8oLykpUa9evTRw4EB973vf09/+9rewtr212nqOp0+f1hVXXCG3260pU6YE/B3F2zV87LHHdOutt+qyyy4LKLfKNQxVS3+D4fjOWiPuNmaMpqqqKkkKuKn5nvteq6qqUq9evQJe79Chg7p37+6vEy3haMtjjz2mwYMHa+zYsQHlDzzwgL7xjW+oc+fO+uMf/6j/+I//0OnTp3XXXXeFrf0taev5zZgxQ1dccYV69+6td955R9///vd16NAhFRcX+9832DX2vRYt4bh+J0+e1IMPPqj58+cHlMfi+p08eVIejyfod3vw4MGgxzR1LS7+e/OVNVUnWtpyfpf6/ve/r969ewfcCCZNmqTCwkJdeeWVOnLkiH7wgx/o+uuvV2lpqZxOZ1jPoSVtOceBAwdq/fr1Gj58uGpra7V69WqNHTtW+/fvV3Z2dlxdw127dum9997TY489FlBupWsYqqb+Buvq6vTZZ5/p73//e7t/71sj7oOXpUuX6ic/+UmzdQ4cOKBBgwZFqUXh19pzbK/PPvtMTz/9tH70ox81eu3isquuukpnzpzRz372s7Dc/CJ9fhffyIcNG6asrCxNmDBBR44cUb9+/dr8vq0VretXV1enG264QUOGDNF9990X8Fokrx/aZtWqVdq4caNKSkoCJrTeeuut/v8eNmyYhg8frn79+qmkpEQTJkyIRVNDkpeXp7y8PP/zsWPHavDgwfrNb36jBx98MIYtC7/HHntMw4YN05gxYwLK7X4NrSDug5d77rlHs2fPbrZO37592/TemZmZkqTq6mplZWX5y6urqzVy5Eh/nRMnTgQcd+HCBdXU1PiPb6/WnmN727Jp0yadPXtWt99+e4t1c3Nz9eCDD+rcuXPt3v8iWufnk5ubK0k6fPiw+vXrp8zMzEYz5aurqyUpLNcwGud36tQpTZo0SV27dtVzzz2njh07Nls/nNevKT169JDT6fR/lz7V1dVNnk9mZmaz9VvzNxktbTk/n9WrV2vVqlX605/+pOHDhzdbt2/fvurRo4cOHz4c9Rtfe87Rp2PHjrrqqqt0+PBhSfFzDc+cOaONGzfqgQceaPFzYnkNQ9XU36DL5VKnTp3kdDrb/TvRKmGbPRNHQp2wu3r1an9ZbW1t0Am7b775pr/OK6+8EtMJu21ty7hx4xqtUmnKj3/8Y9OtW7c2t7UtwvVdv/HGG0aSefvtt40xX0zYvXim/G9+8xvjcrnM559/Hr4TaEFbz6+2ttZ89atfNePGjTNnzpxp1WdF6/qNGTPGLFiwwP/c4/GYPn36NDth95//+Z8DyvLy8hpN2G3ubzKaQj0/Y4z5yU9+YlwulyktLW3VZ5SXlxuHw2FeeOGFdre3Ldpyjhe7cOGCGThwoLn77ruNMfFxDY3x3kdSUlLMyZMnW/yMWF9DH7Vywu7QoUMDyqZPn95owm57fida1dawvVMc+Oijj8zevXv9S4H37t1r9u7dG7AkeODAgaa4uNj/fNWqVSY9Pd288MIL5p133jFTpkwJulT6qquuMjt37jRvvPGGGTBgQEyXSjfXluPHj5uBAweanTt3Bhz34YcfGofDYV5++eVG7/niiy+aRx991Lz77rvmww8/NP/93/9tOnfubJYvXx7x87lUqOd3+PBh88ADD5g333zTlJWVmRdeeMH07dvXXHPNNf5jfEulr7vuOrNv3z6zZcsW07Nnz5gtlQ7l/Gpra01ubq4ZNmyYOXz4cMDSzAsXLhhjYnv9Nm7caFJSUswTTzxh3n//fTN//nyTnp7uX9l12223maVLl/rr79ixw3To0MGsXr3aHDhwwKxYsSLoUumW/iajJdTzW7VqlUlOTjabNm0KuFa+f4NOnTpllixZYkpLS01ZWZn505/+ZK6++mozYMCAqAbS7TnH+++/37zyyivmyJEjZs+ePebWW281qampZv/+/f46dr6GPl//+tfNtGnTGpVb7RqeOnXKf6+TZB5++GGzd+9e89FHHxljjFm6dKm57bbb/PV9S6Xvvfdec+DAAbN27dqgS6Wb+87CgeDlIrNmzTKSGj1ee+01fx39Ix+GT0NDg/nRj35kMjIyTEpKipkwYYI5dOhQwPv+7W9/M9OnTzddunQxLpfLzJkzJyAgiqaW2lJWVtbonI0xZtmyZcbtdhuPx9PoPV9++WUzcuRI06VLF3PZZZeZESNGmHXr1gWtG2mhnt+xY8fMNddcY7p3725SUlJM//79zb333huQ58UYY44ePWquv/5606lTJ9OjRw9zzz33BCw1jpZQz++1114L+jstyZSVlRljYn/9HnnkEXP55Zeb5ORkM2bMGPOXv/zF/9q4cePMrFmzAur//ve/N1/60pdMcnKy+fKXv2z+8Ic/BLzemr/JaArl/K644oqg12rFihXGGGPOnj1rrrvuOtOzZ0/TsWNHc8UVV5h58+aF9abQFqGc46JFi/x1MzIyzOTJk81bb70V8H52vobGGHPw4EEjyfzxj39s9F5Wu4ZN/RvhO6dZs2aZcePGNTpm5MiRJjk52fTt2zfgnujT3HcWDg5joryeFQAAoB3I8wIAAGyF4AUAANgKwQsAALAVghcAAGArBC8AAMBWCF4AAICtELwAAABbIXgBAAC2QvACAABsheAFAADYCsELAACwFYIXAABgK/8f23QHcQ7imPoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_train,y_train,c=\"red\",label=\"train\")\n",
    "plt.scatter(x_test,y_test,c=\"blue\",label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ffabdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.1661 - mse: 0.1661 - val_loss: 0.1491 - val_mse: 0.1491\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.1375 - mse: 0.1375 - val_loss: 0.1241 - val_mse: 0.1241\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1129 - mse: 0.1129 - val_loss: 0.1026 - val_mse: 0.1026\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0917 - mse: 0.0917 - val_loss: 0.0847 - val_mse: 0.0847\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0738 - mse: 0.0738 - val_loss: 0.0700 - val_mse: 0.0700\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0592 - mse: 0.0592 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0476 - mse: 0.0476 - val_loss: 0.0490 - val_mse: 0.0490\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0418 - val_mse: 0.0418\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0287 - val_mse: 0.0287\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0284 - val_mse: 0.0284\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0289 - val_mse: 0.0289\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0307 - val_mse: 0.0307\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0301 - val_mse: 0.0301\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0301 - val_mse: 0.0301\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0301 - val_mse: 0.0301\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0301 - val_mse: 0.0301\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 63/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0307 - val_mse: 0.0307\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0307 - val_mse: 0.0307\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0308 - val_mse: 0.0308\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0312 - val_mse: 0.0312\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0312 - val_mse: 0.0312\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0312 - val_mse: 0.0312\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0324 - val_mse: 0.0324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 186/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0357 - val_mse: 0.0357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 309/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 432/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0353 - val_mse: 0.0353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 494/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 555/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0348 - val_mse: 0.0348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 678/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0352 - val_mse: 0.0352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 740/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 801/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0360 - val_mse: 0.0360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 924/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0375 - val_mse: 0.0375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0376 - val_mse: 0.0376\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0376 - val_mse: 0.0376\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(128,input_dim=1,activation=\"relu\"))\n",
    "model_1.add(Dense(128,activation=\"relu\"))\n",
    "model_1.add(Dense(1,activation=\"linear\"))\n",
    "adam=Adam(learning_rate=0.01)\n",
    "model_1.compile(loss=\"mse\",optimizer=\"Adam\",metrics=[\"mse\"])\n",
    "history=model_1.fit(x_train,y_train,epochs=1000,validation_data=(x_test,y_test),verbose=1)\n",
    "#print(f\"train{train_mse,test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "664c6b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.008976951241493225, Test: 0.03755326196551323\n"
     ]
    }
   ],
   "source": [
    "_, train_mse = model_1.evaluate(x_train, y_train, verbose=0)\n",
    "_, test_mse = model_1.evaluate(x_test, y_test, verbose=0)\n",
    "print('Train: {}, Test: {}'.format(train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12a39891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model_1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6505d369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, c='red', label='Train')\n",
    "plt.scatter(x_test, y_test, c='blue', label='Test')\n",
    "plt.plot(x_test, y_pred)\n",
    "plt.legend()\n",
    "plt.ylim((-1.5, 1.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5a6c959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x166a14c50>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48ab1363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.1678 - mse: 0.1678 - val_loss: 0.1348 - val_mse: 0.1348\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.1051 - mse: 0.1051 - val_loss: 0.1162 - val_mse: 0.1162\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.1139 - mse: 0.1139 - val_loss: 0.1005 - val_mse: 0.1005\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0845 - mse: 0.0845 - val_loss: 0.0862 - val_mse: 0.0862\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0618 - mse: 0.0618 - val_loss: 0.0735 - val_mse: 0.0735\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0782 - mse: 0.0782 - val_loss: 0.0628 - val_mse: 0.0628\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0667 - mse: 0.0667 - val_loss: 0.0536 - val_mse: 0.0536\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.0461 - val_mse: 0.0461\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0509 - mse: 0.0509 - val_loss: 0.0401 - val_mse: 0.0401\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0552 - mse: 0.0552 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0285 - val_mse: 0.0285\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0281 - val_mse: 0.0281\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0281 - val_mse: 0.0281\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0286 - val_mse: 0.0286\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0292 - val_mse: 0.0292\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0307 - val_mse: 0.0307\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0307 - val_mse: 0.0307\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.0301 - val_mse: 0.0301\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0498 - mse: 0.0498 - val_loss: 0.0292 - val_mse: 0.0292\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0288 - val_mse: 0.0288\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.0286 - val_mse: 0.0286\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0286 - val_mse: 0.0286\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0287 - val_mse: 0.0287\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0289 - val_mse: 0.0289\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0293 - val_mse: 0.0293\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0308 - val_mse: 0.0308\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0308 - val_mse: 0.0308\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0301 - val_mse: 0.0301\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 63/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.0307 - val_mse: 0.0307\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0308 - val_mse: 0.0308\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0301 - val_mse: 0.0301\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.0301 - val_mse: 0.0301\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0307 - val_mse: 0.0307\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0318 - val_mse: 0.0318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.0312 - val_mse: 0.0312\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 186/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.0312 - val_mse: 0.0312\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.0312 - val_mse: 0.0312\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0308 - val_mse: 0.0308\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0307 - val_mse: 0.0307\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.0307 - val_mse: 0.0307\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0312 - val_mse: 0.0312\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.0307 - val_mse: 0.0307\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0308 - val_mse: 0.0308\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0312 - val_mse: 0.0312\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0325 - val_mse: 0.0325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 309/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.0332 - val_mse: 0.0332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 432/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0344 - val_mse: 0.0344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 494/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 555/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0318 - val_mse: 0.0318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 678/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0331 - val_mse: 0.0331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 740/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0349 - mse: 0.0349 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 801/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0334 - val_mse: 0.0334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 924/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0334 - val_mse: 0.0334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0340 - val_mse: 0.0340\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128,input_dim=1,activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation=\"linear\"))\n",
    "adam=Adam(learning_rate=0.01)\n",
    "model.compile(loss=\"mse\",optimizer=\"Adam\",metrics=[\"mse\"])\n",
    "history_1=model.fit(x_train,y_train,epochs=1000,validation_data=(x_test,y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7faa299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0154 - mse: 0.0154\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0340 - mse: 0.0340\n",
      "Train: [0.015414243564009666, 0.015414243564009666], Test: [0.03395817428827286, 0.03395817428827286]\n"
     ]
    }
   ],
   "source": [
    "train_mse=model.evaluate(x_train,y_train,verbose=1)\n",
    "test_mse=model.evaluate(x_test,y_test,verbose=1)\n",
    "print('Train: {}, Test: {}'.format(train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f46f571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x168b9a490>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_1.history[\"loss\"])\n",
    "plt.plot(history_1.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942df41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
