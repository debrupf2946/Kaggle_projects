{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4565a301-0869-4963-a96e-1c1d4ee68b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_data=load_dataset(\"lewtun/github-issues\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56a8132e-1742-49ab-bd2c-6a69dd4b3799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3019, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27a36ee0-6072-4a61-94df-21f285adaff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_data[\"is_pull_request\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a02ca16-d2ba-4f6c-96c4-c0577d7dcfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_data=issues_data.filter(lambda x:x[\"is_pull_request\"]==False and len(x[\"comments\"])>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d76ec1f-dcaa-4914-8adb-e647067779f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80fb1600-c799-4daf-b07c-214ed3934314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col=issues_data.column_names\n",
    "col_keep=[\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "col_rem=set(col_keep).symmetric_difference(col)\n",
    "issues_data=issues_data.remove_columns(col_rem)\n",
    "issues_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af0ae7c2-c76d-4512-8db9-eb2a5b30120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_data.set_format(\"pandas\")\n",
    "df=issues_data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ad3fba-80ee-4df3-8071-1ebc1fe34c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>[Cool, I think we can do both :), @lhoestq now...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>[Hi ! I guess the caching mechanism should hav...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>OSCAR unshuffled_original_ko: NonMatchingSplit...</td>\n",
       "      <td>[I tried `unshuffled_original_da` and it is al...</td>\n",
       "      <td>## Describe the bug\\r\\n\\r\\nCannot download OSC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>load_dataset using default cache on Windows ca...</td>\n",
       "      <td>[Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfo...</td>\n",
       "      <td>## Describe the bug\\r\\nStandard process to dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>to_tf_dataset keeps a reference to the open da...</td>\n",
       "      <td>[I did some investigation and, as it seems, th...</td>\n",
       "      <td>To reproduce:\\r\\n```python\\r\\nimport datasets ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/6</td>\n",
       "      <td>Error when citation is not given in the Datase...</td>\n",
       "      <td>[Yes looks good to me.\\r\\nNote that we may ref...</td>\n",
       "      <td>The following error is raised when the `citati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/5</td>\n",
       "      <td>ValueError when a split is empty</td>\n",
       "      <td>[To fix this I propose to modify only the file...</td>\n",
       "      <td>When a split is empty either TEST, VALIDATION ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/4</td>\n",
       "      <td>[Feature] Keep the list of labels of a dataset...</td>\n",
       "      <td>[Yes! I see mostly two options for this:\\r\\n- ...</td>\n",
       "      <td>It would be useful to keep the list of the lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/3</td>\n",
       "      <td>[Feature] More dataset outputs</td>\n",
       "      <td>[Yes!\\r\\n- pandas will be a one-liner in `arro...</td>\n",
       "      <td>Add the following dataset outputs:\\r\\n\\r\\n- Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2</td>\n",
       "      <td>Issue to read a local dataset</td>\n",
       "      <td>[My first bug report ❤️\\r\\nLooking into this r...</td>\n",
       "      <td>Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>808 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              html_url  \\\n",
       "0    https://github.com/huggingface/datasets/issues...   \n",
       "1    https://github.com/huggingface/datasets/issues...   \n",
       "2    https://github.com/huggingface/datasets/issues...   \n",
       "3    https://github.com/huggingface/datasets/issues...   \n",
       "4    https://github.com/huggingface/datasets/issues...   \n",
       "..                                                 ...   \n",
       "803   https://github.com/huggingface/datasets/issues/6   \n",
       "804   https://github.com/huggingface/datasets/issues/5   \n",
       "805   https://github.com/huggingface/datasets/issues/4   \n",
       "806   https://github.com/huggingface/datasets/issues/3   \n",
       "807   https://github.com/huggingface/datasets/issues/2   \n",
       "\n",
       "                                                 title  \\\n",
       "0                                Protect master branch   \n",
       "1    Backwards compatibility broken for cached data...   \n",
       "2    OSCAR unshuffled_original_ko: NonMatchingSplit...   \n",
       "3    load_dataset using default cache on Windows ca...   \n",
       "4    to_tf_dataset keeps a reference to the open da...   \n",
       "..                                                 ...   \n",
       "803  Error when citation is not given in the Datase...   \n",
       "804                   ValueError when a split is empty   \n",
       "805  [Feature] Keep the list of labels of a dataset...   \n",
       "806                     [Feature] More dataset outputs   \n",
       "807                      Issue to read a local dataset   \n",
       "\n",
       "                                              comments  \\\n",
       "0    [Cool, I think we can do both :), @lhoestq now...   \n",
       "1    [Hi ! I guess the caching mechanism should hav...   \n",
       "2    [I tried `unshuffled_original_da` and it is al...   \n",
       "3    [Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfo...   \n",
       "4    [I did some investigation and, as it seems, th...   \n",
       "..                                                 ...   \n",
       "803  [Yes looks good to me.\\r\\nNote that we may ref...   \n",
       "804  [To fix this I propose to modify only the file...   \n",
       "805  [Yes! I see mostly two options for this:\\r\\n- ...   \n",
       "806  [Yes!\\r\\n- pandas will be a one-liner in `arro...   \n",
       "807  [My first bug report ❤️\\r\\nLooking into this r...   \n",
       "\n",
       "                                                  body  \n",
       "0    After accidental merge commit (91c55355b634d0d...  \n",
       "1    ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "2    ## Describe the bug\\r\\n\\r\\nCannot download OSC...  \n",
       "3    ## Describe the bug\\r\\nStandard process to dow...  \n",
       "4    To reproduce:\\r\\n```python\\r\\nimport datasets ...  \n",
       "..                                                 ...  \n",
       "803  The following error is raised when the `citati...  \n",
       "804  When a split is empty either TEST, VALIDATION ...  \n",
       "805  It would be useful to keep the list of the lab...  \n",
       "806  Add the following dataset outputs:\\r\\n\\r\\n- Sp...  \n",
       "807  Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...  \n",
       "\n",
       "[808 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31f9370d-79fe-426d-97a6-34998e7defba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cool, I think we can do both :)',\n",
       " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comments\"][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e443a6d3-5839-4c86-a80b-40dd19721306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Well it can cause issue with anyone that updat...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1                              Protect master branch   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "4  Backwards compatibility broken for cached data...   \n",
       "\n",
       "                                            comments  \\\n",
       "0                    Cool, I think we can do both :)   \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
       "2  Hi ! I guess the caching mechanism should have...   \n",
       "3  If it's easy enough to implement, then yes ple...   \n",
       "4  Well it can cause issue with anyone that updat...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  After accidental merge commit (91c55355b634d0d...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "4  ## Describe the bug\\r\\nAfter upgrading to data...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df=df.explode(\"comments\",ignore_index=True)\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05d970f6-f13e-4cfa-9114-3e8739dcc339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "# comments_dataset=Dataset.from_pandas(comments_df)\n",
    "# comments_dataset\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea9965ae-1a40-455f-b6d7-4a34ce07fe6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32999df5ebf8424b9bb482cae1c727eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset=comments_dataset.map(lambda x:{\"comment_length\":len(x[\"comments\"].split())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5900f6d-9754-499c-a9b4-ab09ac7bcbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35811d187d44404a962d8582e8e65050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset=comments_dataset.filter(lambda x:x[\"comment_length\"]>15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1aae3fd-473b-4393-a5f0-fc40a8ac4baa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c6ecf145fc4b608797b29c343b8bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset=comments_dataset.map(lambda x:{\n",
    "    \"text\":x[\"title\"]\n",
    "    + \"\\n\"\n",
    "    + x[\"body\"]\n",
    "    + \"\\n\"\n",
    "    + x[\"comments\"]\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ff1b227-9914-4899-a822-8ef1d4febfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 01:53:01.506241: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-09-24 01:53:01.506261: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2023-09-24 01:53:01.506270: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2023-09-24 01:53:01.506893: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-09-24 01:53:01.507050: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFMPNetModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,TFAutoModel\n",
    "model_ckpt=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model=TFAutoModel.from_pretrained(model_ckpt,from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21bee2ae-3d68-4c48-a966-62d6ca363dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f93e4415-5f02-4462-aca5-cf56ed0288f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input=tokenizer(comments_dataset[\"text\"][:5],padding=True, truncation=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d585c956-846a-4001-828f-003c8376fd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(5, 512), dtype=int32, numpy=\n",
       "array([[    0,  4051,  3044, ...,     1,     1,     1],\n",
       "       [    0, 11047, 21782, ...,  1016, 17057,     2],\n",
       "       [    0, 11047, 21782, ...,  1016, 17057,     2],\n",
       "       [    0, 11047, 21782, ...,  1016, 17057,     2],\n",
       "       [    0, 11047, 21782, ...,  1016, 17057,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(5, 512), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76d91f90-29b7-4c34-9b26-40e6687c04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = {k: v for k, v in encoded_input.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a0074a3-d7f5-4272-a4ac-a249160cbc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(5, 512), dtype=int32, numpy=\n",
       " array([[    0,  4051,  3044, ...,     1,     1,     1],\n",
       "        [    0, 11047, 21782, ...,  1016, 17057,     2],\n",
       "        [    0, 11047, 21782, ...,  1016, 17057,     2],\n",
       "        [    0, 11047, 21782, ...,  1016, 17057,     2],\n",
       "        [    0, 11047, 21782, ...,  1016, 17057,     2]], dtype=int32)>,\n",
       " 'attention_mask': <tf.Tensor: shape=(5, 512), dtype=int32, numpy=\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35e20bd3-0623-4031-8c29-158ce15d5723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor: shape=(5, 512, 768), dtype=float32, numpy=\n",
       "array([[[-1.55319750e-01, -1.00230552e-01, -7.03210235e-02, ...,\n",
       "         -1.52763486e-01,  6.11720756e-02, -1.98785812e-01],\n",
       "        [-8.91083330e-02,  1.13443509e-01, -1.67645380e-01, ...,\n",
       "         -1.86630547e-01,  7.04902709e-02, -1.66866958e-01],\n",
       "        [-1.29643872e-01, -1.22836448e-01, -1.30288675e-01, ...,\n",
       "         -3.26714963e-01, -6.72660768e-04, -5.60678206e-02],\n",
       "        ...,\n",
       "        [ 6.51642084e-02, -6.04542121e-02, -1.94934785e-01, ...,\n",
       "         -2.19161324e-02, -1.22386664e-01, -1.26843691e-01],\n",
       "        [ 6.52385950e-02, -5.86896464e-02, -1.94787800e-01, ...,\n",
       "         -2.20095664e-02, -1.22267865e-01, -1.27117902e-01],\n",
       "        [ 6.58856034e-02, -5.79189733e-02, -1.94597036e-01, ...,\n",
       "         -2.21509039e-02, -1.22761920e-01, -1.27215356e-01]],\n",
       "\n",
       "       [[-3.04995358e-01,  1.24387980e-01, -4.65585105e-02, ...,\n",
       "         -8.82679671e-02,  2.77353525e-01, -1.29942030e-01],\n",
       "        [-1.70043319e-01,  4.60807383e-02, -1.14994839e-01, ...,\n",
       "         -1.55956224e-02,  2.77617872e-01, -1.78309865e-02],\n",
       "        [-1.01464763e-01,  1.02745026e-01, -1.97466224e-01, ...,\n",
       "          1.34883076e-02,  2.51092017e-01,  6.48831204e-02],\n",
       "        ...,\n",
       "        [-6.77378699e-02, -8.23049188e-01, -1.87504992e-01, ...,\n",
       "          1.43233478e-01,  1.63959771e-01, -7.29688779e-02],\n",
       "        [-1.16026521e-01,  4.12766933e-01, -2.05837041e-01, ...,\n",
       "          1.32483035e-01, -3.57024372e-04, -1.30448371e-01],\n",
       "        [-2.88509756e-01, -3.95944715e-03, -8.01264793e-02, ...,\n",
       "         -4.16813269e-02,  3.31376255e-01, -1.24576755e-01]],\n",
       "\n",
       "       [[-3.04995358e-01,  1.24387980e-01, -4.65585105e-02, ...,\n",
       "         -8.82679671e-02,  2.77353525e-01, -1.29942030e-01],\n",
       "        [-1.70043319e-01,  4.60807383e-02, -1.14994839e-01, ...,\n",
       "         -1.55956224e-02,  2.77617872e-01, -1.78309865e-02],\n",
       "        [-1.01464763e-01,  1.02745026e-01, -1.97466224e-01, ...,\n",
       "          1.34883076e-02,  2.51092017e-01,  6.48831204e-02],\n",
       "        ...,\n",
       "        [-6.77378699e-02, -8.23049188e-01, -1.87504992e-01, ...,\n",
       "          1.43233478e-01,  1.63959771e-01, -7.29688779e-02],\n",
       "        [-1.16026521e-01,  4.12766933e-01, -2.05837041e-01, ...,\n",
       "          1.32483035e-01, -3.57024372e-04, -1.30448371e-01],\n",
       "        [-2.88509756e-01, -3.95944715e-03, -8.01264793e-02, ...,\n",
       "         -4.16813269e-02,  3.31376255e-01, -1.24576755e-01]],\n",
       "\n",
       "       [[-3.04995358e-01,  1.24387980e-01, -4.65585105e-02, ...,\n",
       "         -8.82679671e-02,  2.77353525e-01, -1.29942030e-01],\n",
       "        [-1.70043319e-01,  4.60807383e-02, -1.14994839e-01, ...,\n",
       "         -1.55956224e-02,  2.77617872e-01, -1.78309865e-02],\n",
       "        [-1.01464763e-01,  1.02745026e-01, -1.97466224e-01, ...,\n",
       "          1.34883076e-02,  2.51092017e-01,  6.48831204e-02],\n",
       "        ...,\n",
       "        [-6.77378699e-02, -8.23049188e-01, -1.87504992e-01, ...,\n",
       "          1.43233478e-01,  1.63959771e-01, -7.29688779e-02],\n",
       "        [-1.16026521e-01,  4.12766933e-01, -2.05837041e-01, ...,\n",
       "          1.32483035e-01, -3.57024372e-04, -1.30448371e-01],\n",
       "        [-2.88509756e-01, -3.95944715e-03, -8.01264793e-02, ...,\n",
       "         -4.16813269e-02,  3.31376255e-01, -1.24576755e-01]],\n",
       "\n",
       "       [[-3.04995358e-01,  1.24387980e-01, -4.65585105e-02, ...,\n",
       "         -8.82679671e-02,  2.77353525e-01, -1.29942030e-01],\n",
       "        [-1.70043319e-01,  4.60807383e-02, -1.14994839e-01, ...,\n",
       "         -1.55956224e-02,  2.77617872e-01, -1.78309865e-02],\n",
       "        [-1.01464763e-01,  1.02745026e-01, -1.97466224e-01, ...,\n",
       "          1.34883076e-02,  2.51092017e-01,  6.48831204e-02],\n",
       "        ...,\n",
       "        [-6.77378699e-02, -8.23049188e-01, -1.87504992e-01, ...,\n",
       "          1.43233478e-01,  1.63959771e-01, -7.29688779e-02],\n",
       "        [-1.16026521e-01,  4.12766933e-01, -2.05837041e-01, ...,\n",
       "          1.32483035e-01, -3.57024372e-04, -1.30448371e-01],\n",
       "        [-2.88509756e-01, -3.95944715e-03, -8.01264793e-02, ...,\n",
       "         -4.16813269e-02,  3.31376255e-01, -1.24576755e-01]]],\n",
       "      dtype=float32)>, pooler_output=<tf.Tensor: shape=(5, 768), dtype=float32, numpy=\n",
       "array([[-0.05430851,  0.11577124,  0.04738923, ...,  0.17078935,\n",
       "         0.18965857,  0.06086588],\n",
       "       [ 0.15239197, -0.04347897,  0.19614616, ...,  0.04516955,\n",
       "        -0.02733935,  0.08677382],\n",
       "       [ 0.15239197, -0.04347897,  0.19614616, ...,  0.04516955,\n",
       "        -0.02733935,  0.08677382],\n",
       "       [ 0.15239197, -0.04347897,  0.19614616, ...,  0.04516955,\n",
       "        -0.02733935,  0.08677382],\n",
       "       [ 0.15239197, -0.04347897,  0.19614616, ...,  0.04516955,\n",
       "        -0.02733935,  0.08677382]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae77d44f-4319-4377-a87c-56e3ff6785be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('input_ids', <tf.Tensor: shape=(5, 512), dtype=int32, numpy=\n",
       "array([[    0,  4051,  3044, ...,     1,     1,     1],\n",
       "       [    0, 11047, 21782, ...,  1016, 17057,     2],\n",
       "       [    0, 11047, 21782, ...,  1016, 17057,     2],\n",
       "       [    0, 11047, 21782, ...,  1016, 17057,     2],\n",
       "       [    0, 11047, 21782, ...,  1016, 17057,     2]], dtype=int32)>), ('attention_mask', <tf.Tensor: shape=(5, 512), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cb5dec2-afef-44fc-b4da-38f88cdeeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input=tokenizer(text_list,padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    encoded_input = {k: v for k, v in encoded_input.items()}\n",
    "    model_output=model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "acfb5df5-e94c-47f9-aeca-33cf6679cfd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 768])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952702f-64a5-4d5f-884b-a58b0f1462aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x2950bf880> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6d7b0fb8454bf3a080aacc2be3ac72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dataset=comments_dataset.map(lambda x:{\"embeddings\":get_embeddings(x[\"text\"]).numpy()[0]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d16f38-c5f5-4504-ae75-7fb6a77fcfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c9c7db-3835-404e-b961-70b146634408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder,load_dataset\n",
    "\n",
    "# Replace 'path/to/saved/dataset' with the path to the directory where you saved the dataset\n",
    "builder = load_dataset_builder(\"/Users/debruppaul/Untitled Folder/transformers-course/datasets/embedings_fasis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f0046fa-5291-4d8c-9620-8adf33cff26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dataset=load_dataset(\"json\",data_files=\"/Users/debruppaul/Downloads/data_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d897c4e-375d-4ebb-8019-970ad9957271",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_data=embedding_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937921d5-6343-40b0-90c6-7124aca74692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfec81fb-0840-49f1-8526-eaf2076720cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9e415be39e46afbea9c30b2822e625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_data.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6c9a32b-a380-4602-808b-7bb6bc119ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10d66641-a557-4aec-a005-2ec48cf80e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores,samples=embedding_data.get_nearest_examples(\"embeddings\",question_embedding,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d02bbb7-da6a-4de5-8231-f4dc7c9b2307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9086330d-ea8b-48d0-ab37-ee86f9b9ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea442dc7-165b-418b-97b2-13df33e7724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df=pd.DataFrame.from_dict(samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0a3921d-3f3e-4d8f-a2ee-4fddbec1f048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   html_url        5 non-null      object\n",
      " 1   title           5 non-null      object\n",
      " 2   comments        5 non-null      object\n",
      " 3   body            5 non-null      object\n",
      " 4   comment_length  5 non-null      int64 \n",
      " 5   text            5 non-null      object\n",
      " 6   embeddings      5 non-null      object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 412.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "samples_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f1a8f93-2ecb-4e11-832b-b1e6ccdf0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df[\"scores\"]=scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2353ceae-0288-4e87-bc43-56bacd061b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af5f68ef-e853-4f63-b2d3-c1bd950f393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
      "\n",
      "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "SCORE: 25.505020141601562\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
      "You can now use them offline\n",
      "```python\n",
      "datasets = load_dataset('text', data_files=data_files)\n",
      "```\n",
      "\n",
      "We'll do a new release soon\n",
      "SCORE: 24.55553436279297\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
      "\n",
      "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \n",
      "\n",
      "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
      "\n",
      "----------\n",
      "\n",
      "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "\n",
      "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
      "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
      "```python\n",
      "load_dataset(\"./my_dataset\")\n",
      "```\n",
      "and the dataset script will generate your dataset once and for all.\n",
      "\n",
      "----------\n",
      "\n",
      "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
      "cf #1724 \n",
      "SCORE: 24.148983001708984\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
      "> \n",
      "> 1. (online machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_dataset(...)\n",
      "> \n",
      "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> 2. copy the dir from online to the offline machine\n",
      "> \n",
      "> 3. (offline machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> \n",
      "> \n",
      "> HTH.\n",
      "\n",
      "\n",
      "SCORE: 22.89400291442871\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
      "1. (online machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_dataset(...)\n",
      "data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "```\n",
      "2. copy the dir from online to the offline machine\n",
      "3. (offline machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "```\n",
      "\n",
      "HTH.\n",
      "SCORE: 22.40664291381836\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95790c7a-d340-4e03-86f8-aacafaf06b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
